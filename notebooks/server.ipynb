{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMYdd2wZeD7i"
      },
      "outputs": [],
      "source": [
        "!pip install -q --ignore-installed llama-cpp-python flask flask-cors requests pyngrok python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwvzKkIWhQM0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OSJMVKuhiI4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import torch\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from llama_cpp import Llama\n",
        "from pathlib import Path\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "llm = None\n",
        "model_info = {}\n",
        "\n",
        "def initialize_model(model_path, n_gpu_layers=35):\n",
        "    global llm, model_info\n",
        "\n",
        "    logger.info(f\"Initializing Phi-3 GGUF Model\")\n",
        "    logger.info(f\"Model: {model_path}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "        logger.info(f\"GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
        "\n",
        "    llm = Llama(\n",
        "        model_path=model_path,\n",
        "        n_ctx=4096,  # Larger context for multi-step reasoning change according to your convenience\n",
        "        n_threads=4,\n",
        "        n_gpu_layers=n_gpu_layers,\n",
        "        verbose=False,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "\n",
        "    model_info = {\n",
        "        'loaded': True,\n",
        "        'model_path': model_path,\n",
        "        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
        "        'gpu_memory': torch.cuda.get_device_properties(0).total_memory / (1024**3) if torch.cuda.is_available() else 0,\n",
        "    }\n",
        "\n",
        "    logger.info(\"âœ… Model loaded successfully!\")\n",
        "    return True\n",
        "\n",
        "def format_prompt(user_message: str, context: dict = None, requesting_player: str = None) -> str:\n",
        "    \"\"\"Format message for Phi-3 with enhanced reasoning\"\"\"\n",
        "\n",
        "    system_prompt = f\"\"\"You are an intelligent Minecraft bot assistant. You can understand natural language, break down complex requests into multiple steps, and reason about what needs to be done.\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "1. \"me\" or \"I\" ALWAYS refers to the player who is talking to you: {requesting_player or \"the player\"}\n",
        "2. You can execute MULTIPLE actions in sequence - return them as an array\n",
        "3. Think step-by-step: if player asks for something you don't have, first get it, then give it\n",
        "4. Be conversational for greetings/questions, but also provide helpful context\n",
        "\n",
        "AVAILABLE ACTIONS:\n",
        "1. mine: {{\"action\":\"mine\",\"blockType\":\"string\",\"count\":number}}\n",
        "2. fight: {{\"action\":\"fight\",\"mobType\":\"string\",\"radius\":number}}\n",
        "3. harvest: {{\"action\":\"harvest\",\"cropType\":\"string\",\"radius\":number}}\n",
        "4. craft: {{\"action\":\"craft\",\"itemName\":\"string\",\"count\":number}}\n",
        "5. navigate: {{\"action\":\"navigate\",\"x\":number,\"y\":number,\"z\":number}}\n",
        "6. follow: {{\"action\":\"follow\",\"playerName\":\"string\",\"distance\":number}}\n",
        "7. give: {{\"action\":\"give\",\"playerName\":\"string\",\"itemName\":\"string\",\"count\":number}}\n",
        "8. afk: {{\"action\":\"afk\",\"duration\":number}}\n",
        "9. status: {{\"action\":\"status\"}}\n",
        "10. stop: {{\"action\":\"stop\"}}\n",
        "11. respond: {{\"action\":\"respond\",\"message\":\"string\"}} - For conversations/greetings\n",
        "\n",
        "RESPONSE FORMATS:\n",
        "\n",
        "For simple greetings/questions:\n",
        "{{\"action\":\"respond\",\"message\":\"I'm doing great! Ready to help you mine, build, or fight!\"}}\n",
        "\n",
        "For single action:\n",
        "{{\"action\":\"mine\",\"blockType\":\"diamond_ore\",\"count\":5}}\n",
        "\n",
        "For multiple actions (IMPORTANT - use steps array):\n",
        "{{\n",
        "  \"steps\": [\n",
        "    {{\"action\":\"mine\",\"blockType\":\"oak_log\",\"count\":2}},\n",
        "    {{\"action\":\"give\",\"playerName\":\"{requesting_player or 'player'}\",\"itemName\":\"oak_log\",\"count\":2}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "For complex requests requiring reasoning:\n",
        "{{\n",
        "  \"steps\": [\n",
        "    {{\"action\":\"mine\",\"blockType\":\"diamond_ore\",\"count\":3}},\n",
        "    {{\"action\":\"craft\",\"itemName\":\"diamond_pickaxe\",\"count\":1}},\n",
        "    {{\"action\":\"give\",\"playerName\":\"{requesting_player or 'player'}\",\"itemName\":\"diamond_pickaxe\",\"count\":1}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "EXAMPLES:\n",
        "\n",
        "User: \"how are you?\"\n",
        "Response: {{\"action\":\"respond\",\"message\":\"I'm doing great! Ready to help you with mining, crafting, or fighting!\"}}\n",
        "\n",
        "User: \"mine 2 logs and give them to me\"\n",
        "Response: {{\n",
        "  \"steps\": [\n",
        "    {{\"action\":\"mine\",\"blockType\":\"oak_log\",\"count\":2}},\n",
        "    {{\"action\":\"give\",\"playerName\":\"{requesting_player or 'player'}\",\"itemName\":\"oak_log\",\"count\":2}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "User: \"I need a diamond pickaxe\"\n",
        "Response: {{\n",
        "  \"steps\": [\n",
        "    {{\"action\":\"mine\",\"blockType\":\"diamond_ore\",\"count\":3}},\n",
        "    {{\"action\":\"craft\",\"itemName\":\"diamond_pickaxe\",\"count\":1}},\n",
        "    {{\"action\":\"give\",\"playerName\":\"{requesting_player or 'player'}\",\"itemName\":\"diamond_pickaxe\",\"count\":1}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "User: \"get me some wood\"\n",
        "Response: {{\n",
        "  \"steps\": [\n",
        "    {{\"action\":\"mine\",\"blockType\":\"oak_log\",\"count\":5}},\n",
        "    {{\"action\":\"give\",\"playerName\":\"{requesting_player or 'player'}\",\"itemName\":\"oak_log\",\"count\":5}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "User: \"craft sticks and give to Steve\"\n",
        "Response: {{\n",
        "  \"steps\": [\n",
        "    {{\"action\":\"craft\",\"itemName\":\"stick\",\"count\":4}},\n",
        "    {{\"action\":\"give\",\"playerName\":\"Steve\",\"itemName\":\"stick\",\"count\":4}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Remember:\n",
        "- \"me\", \"I\", \"my\" = {requesting_player or \"the player who is talking\"}\n",
        "- Think about dependencies (need materials before crafting)\n",
        "- Break complex requests into logical steps\n",
        "- Respond conversationally to greetings\n",
        "- ALWAYS return valid JSON only, no explanations\"\"\"\n",
        "\n",
        "    context_str = \"\"\n",
        "    if context:\n",
        "        context_str = f\"\\nBot Status: Health={context.get('health', 20)}/20, Food={context.get('food', 20)}/20, Position={context.get('position', 'unknown')}\"\n",
        "\n",
        "    prompt = f\"\"\"<|system|>\n",
        "{system_prompt}<|end|>\n",
        "<|user|>\n",
        "Player {requesting_player or 'Unknown'} says: {user_message}{context_str}<|end|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def extract_json(text: str):\n",
        "    \"\"\"Extract JSON from response, handling both single and multi-step formats\"\"\"\n",
        "    try:\n",
        "        # Remove markdown code blocks if present\n",
        "        text = re.sub(r'```json\\s*|\\s*```', '', text)\n",
        "\n",
        "        # Find JSON object\n",
        "        start_idx = text.find('{')\n",
        "        end_idx = text.rfind('}') + 1\n",
        "\n",
        "        if start_idx >= 0 and end_idx > start_idx:\n",
        "            json_str = text[start_idx:end_idx]\n",
        "            parsed = json.loads(json_str)\n",
        "            return parsed\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"JSON extraction error: {e}\")\n",
        "        return None\n",
        "\n",
        "def resolve_player_references(command, requesting_player):\n",
        "    \"\"\"Replace 'me' and 'I' references with actual player name\"\"\"\n",
        "    if isinstance(command, dict):\n",
        "        for key, value in command.items():\n",
        "            if isinstance(value, str):\n",
        "                if value.lower() in ['me', 'i', 'myself']:\n",
        "                    command[key] = requesting_player\n",
        "    return command\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\n",
        "        'status': 'online',\n",
        "        'model_loaded': llm is not None,\n",
        "        'gpu_available': torch.cuda.is_available(),\n",
        "        'model_info': model_info\n",
        "    })\n",
        "\n",
        "@app.route('/parse', methods=['POST'])\n",
        "def parse_command():\n",
        "    if llm is None:\n",
        "        return jsonify({\n",
        "            'error': 'Model not loaded',\n",
        "            'action': None,\n",
        "            'params': {},\n",
        "            'steps': []\n",
        "        }), 503\n",
        "\n",
        "    data = request.json\n",
        "    text = data.get('text', '')\n",
        "    context = data.get('context', {})\n",
        "    requesting_player = data.get('playerName', 'Player')\n",
        "\n",
        "    if not text:\n",
        "        return jsonify({\n",
        "            'error': 'No text',\n",
        "            'action': None,\n",
        "            'params': {},\n",
        "            'steps': []\n",
        "        }), 400\n",
        "\n",
        "    logger.info(f\"ðŸ“¥ Parsing from {requesting_player}: {text}\")\n",
        "\n",
        "    try:\n",
        "        prompt = format_prompt(text, context, requesting_player)\n",
        "\n",
        "        # Generate response\n",
        "        output = llm(\n",
        "            prompt,\n",
        "            max_tokens=500,  # More tokens for multi-step\n",
        "            stop=[\"<|end|>\"],\n",
        "            echo=False,\n",
        "        )\n",
        "\n",
        "        response = output[\"choices\"][0][\"text\"].strip()\n",
        "        logger.info(f\"ðŸ¤– Raw response: {response[:200]}\")\n",
        "\n",
        "        # Extract JSON\n",
        "        parsed = extract_json(response)\n",
        "\n",
        "        if not parsed:\n",
        "            return jsonify({\n",
        "                'error': 'Could not parse response',\n",
        "                'action': None,\n",
        "                'params': {},\n",
        "                'steps': [],\n",
        "                'raw_response': response[:200]\n",
        "            }), 400\n",
        "\n",
        "        # Handle multi-step commands\n",
        "        if 'steps' in parsed:\n",
        "            logger.info(f\"Multi-step command with {len(parsed['steps'])} steps\")\n",
        "\n",
        "            # Resolve player references in all steps\n",
        "            steps = []\n",
        "            for step in parsed['steps']:\n",
        "                resolved_step = resolve_player_references(step, requesting_player)\n",
        "                steps.append(resolved_step)\n",
        "\n",
        "            return jsonify({\n",
        "                'action': None,\n",
        "                'params': {},\n",
        "                'steps': steps,\n",
        "                'error': None,\n",
        "                'is_multistep': True\n",
        "            })\n",
        "\n",
        "        # Handle single action\n",
        "        action = parsed.get('action')\n",
        "\n",
        "        if not action:\n",
        "            return jsonify({\n",
        "                'error': 'No action in response',\n",
        "                'action': None,\n",
        "                'params': {},\n",
        "                'steps': []\n",
        "            }), 400\n",
        "\n",
        "        # Resolve player references\n",
        "        resolved = resolve_player_references(parsed, requesting_player)\n",
        "\n",
        "        # Extract params\n",
        "        params = {k: v for k, v in resolved.items() if k != 'action'}\n",
        "\n",
        "        logger.info(f\"Single action: {action}\")\n",
        "\n",
        "        return jsonify({\n",
        "            'action': action,\n",
        "            'params': params,\n",
        "            'steps': [],\n",
        "            'error': None,\n",
        "            'is_multistep': False\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error: {e}\")\n",
        "        return jsonify({\n",
        "            'error': str(e),\n",
        "            'action': None,\n",
        "            'params': {},\n",
        "            'steps': []\n",
        "        }), 500\n",
        "\n",
        "@app.route('/info', methods=['GET'])\n",
        "def info():\n",
        "    return jsonify(model_info)\n",
        "\n",
        "@app.route('/stats', methods=['GET'])\n",
        "def stats():\n",
        "    if not torch.cuda.is_available():\n",
        "        return jsonify({'gpu_available': False})\n",
        "\n",
        "    return jsonify({\n",
        "        'gpu_available': True,\n",
        "        'gpu_name': torch.cuda.get_device_name(0),\n",
        "        'gpu_memory_total': torch.cuda.get_device_properties(0).total_memory / (1024**3),\n",
        "        'gpu_memory_allocated': torch.cuda.memory_allocated() / (1024**3),\n",
        "        'gpu_memory_cached': torch.cuda.memory_reserved() / (1024**3),\n",
        "    })"
      ],
      "metadata": {
        "id": "Pw4V45pLljWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXsIBpEOlDcF"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def run_server_with_ngrok(model_path, ngrok_token):\n",
        "    \"\"\"Setup and run server with ngrok tunnel\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ADVANCED Phi-3 Server - Multi-Step Reasoning\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Check model\n",
        "    if not Path(model_path).exists():\n",
        "        print(f\"Model not found: {model_path}\")\n",
        "        print(\"\\n Upload options:\")\n",
        "        print(\"   1. Upload via Files tab\")\n",
        "        print(\"   2. Mount Drive: /content/drive/MyDrive/phi3_model.gguf\")\n",
        "        return\n",
        "\n",
        "    # Load model\n",
        "    print(\"Loading model...\")\n",
        "    if not initialize_model(model_path, n_gpu_layers=35):\n",
        "        print(\"Failed to load model\")\n",
        "        return\n",
        "\n",
        "    # Setup ngrok\n",
        "    print(\"\\n Setting up ngrok tunnel...\")\n",
        "    if ngrok_token:\n",
        "        ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "    # Start ngrok tunnel\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\" TUNNEL READY!\")\n",
        "    print(f\" Public URL: {public_url}\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\n COPY THIS URL TO YOUR LOCAL .env FILE:\")\n",
        "    print(f\"    COLAB_SERVER_URL={public_url}\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"\\n Features enabled:\")\n",
        "    print(\"   âœ“ Natural conversation (greetings, questions)\")\n",
        "    print(\"   âœ“ Multi-step command processing\")\n",
        "    print(\"   âœ“ 'me' reference resolution\")\n",
        "    print(\"   âœ“ Smart crafting (auto-gather materials)\")\n",
        "    print(\"   âœ“ Complex request reasoning\")\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Start Flask\n",
        "    print(\" Starting Flask server...\")\n",
        "\n",
        "    from werkzeug.serving import run_simple\n",
        "    run_simple('0.0.0.0', 5000, app, use_reloader=False, use_debugger=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS-hWlAthkBY"
      },
      "outputs": [],
      "source": [
        "# 1. Set your model path\n",
        "MODEL_PATH = 'path to your model'  # If uploaded to Colab\n",
        "\n",
        "# 2. Set your ngrok auth token\n",
        "NGROK_TOKEN = 'your-ngrok-token'  # Get from https://dashboard.ngrok.com\n",
        "\n",
        "run_server_with_ngrok(MODEL_PATH, NGROK_TOKEN)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}