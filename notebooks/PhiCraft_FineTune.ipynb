{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-section"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "!pip install transformers peft accelerate sentencepiece huggingface_hub\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install bitsandbytes==0.48.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mount-section"
      },
      "source": [
        "## Mount Google Drive & Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-section"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config-cell"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "OUTPUT_DIR = \"./minecraft-phi3-bot\"\n",
        "\n",
        "# LoRA Parameters\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Training Parameters\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 2\n",
        "GRADIENT_ACCUMULATION_STEPS = 8\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "WARMUP_STEPS = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset-section"
      },
      "source": [
        "## Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-dataset"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('json', data_files='minecraft_dataset_cleaned.jsonl', split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "format-section"
      },
      "source": [
        "## Format Data for Phi-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "format-data"
      },
      "outputs": [],
      "source": [
        "def format_phi3_prompt(example):\n",
        "    \"\"\"\n",
        "    Format data for Phi-3 using its chat template:\n",
        "    <|user|>\\nUser message<|end|>\\n<|assistant|>\\nAssistant response<|end|>\n",
        "    \"\"\"\n",
        "\n",
        "    # prompt + response\n",
        "    if 'prompt' in example and 'response' in example:\n",
        "        prompt = example['prompt']\n",
        "        response = example['response']\n",
        "\n",
        "        # Skip if None\n",
        "        if prompt is None or response is None:\n",
        "            return {\"text\": \"\"}\n",
        "\n",
        "        prompt = prompt.strip()\n",
        "        response = response.strip()\n",
        "\n",
        "        # Add memory context if present\n",
        "        memory_note = \"\"\n",
        "        if example.get('memoryRecall'):\n",
        "            memory_note = \" [Using Memory]\"\n",
        "\n",
        "        # Phi-3 chat format\n",
        "        text = f\"\"\"<|system|>\n",
        "You are an expert Minecraft assistant with memory capabilities. You help players with mining, crafting, building, combat, farming, and navigation. You can recall past events and locations.<|end|>\n",
        "<|user|>\n",
        "{prompt}{memory_note}<|end|>\n",
        "<|assistant|>\n",
        "{response}<|end|>\"\"\"\n",
        "\n",
        "    # plan (multi-step actions)\n",
        "    elif 'plan' in example and example['plan'] is not None:\n",
        "        plan = example['plan']\n",
        "        plan_json = json.dumps(plan, indent=2)\n",
        "\n",
        "        # Extract task description\n",
        "        first_action = plan[0] if plan else {}\n",
        "        module = first_action.get('module', 'action')\n",
        "        action = first_action.get('action', 'execute')\n",
        "\n",
        "        text = f\"\"\"<|system|>\n",
        "You are an expert Minecraft assistant. Create detailed action plans.<|end|>\n",
        "<|user|>\n",
        "Execute {module} {action}<|end|>\n",
        "<|assistant|>\n",
        "{plan_json}<|end|>\"\"\"\n",
        "\n",
        "    # FALLBACK\n",
        "    else:\n",
        "        text = f\"<|system|>You are a Minecraft assistant.<|end|><|user|>{str(example)}<|end|><|assistant|><|end|>\"\n",
        "\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Apply formatting\n",
        "dataset = dataset.map(format_phi3_prompt, remove_columns=dataset.column_names)\n",
        "\n",
        "# Remove empty examples\n",
        "dataset = dataset.filter(lambda x: len(x['text']) > 10)\n",
        "\n",
        "# Split into train/validation\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-section"
      },
      "source": [
        "## Load Phi-3 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-model"
      },
      "outputs": [],
      "source": [
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    add_eos_token=True,\n",
        "    use_fast=True\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"eager\",\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmpvicXWIzJ8"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    result = tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        padding=False,\n",
        "    )\n",
        "    return result\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Tokenizing training data\"\n",
        ")\n",
        "\n",
        "eval_dataset = eval_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=eval_dataset.column_names,\n",
        "    desc=\"Tokenizing validation data\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lora-section"
      },
      "source": [
        "## Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "configure-lora"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "\n",
        "# Configuring LoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training-config-section"
      },
      "source": [
        "## Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training-config"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    group_by_length=True,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    ddp_find_unused_parameters=False,\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trainer-section"
      },
      "source": [
        "## Initialize Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init-trainer"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# data collator with padding\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    pad_to_multiple_of=8,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train-section"
      },
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start-training"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save-section"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-model"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test-section"
      },
      "source": [
        "### Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test-model"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import gc\n",
        "\n",
        "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "ADAPTER_PATH = \"/content/drive/MyDrive/CraftAI/minecraft-phi3-bot\"\n",
        "\n",
        "# Clear memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Loading Model\n",
        "try:\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "except RuntimeError as e:\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"cpu\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "model.eval()\n",
        "\n",
        "# Loading tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "def generate_response(prompt, max_tokens=150):\n",
        "    \"\"\"Generate response from Phi-3\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            use_cache = False        )\n",
        "\n",
        "    # Decoding only the necessary tokens (excluding input prompt)\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test prompts\n",
        "test_cases = [\n",
        "    \"Check if we have enough sticks. We ran out yesterday.\",\n",
        "    \"Go back to the iron cave we found yesterday and mine more.\",\n",
        "    \"Defend the farm again from those mobs.\",\n",
        "    \"Mine 5 iron ores and return to me.\",\n",
        "    \"Craft a diamond pickaxe.\",\n",
        "]\n",
        "\n",
        "\n",
        "for i, user_message in enumerate(test_cases, 1):\n",
        "    print(f\"Test {i}/{len(test_cases)}\")\n",
        "    print(f\"User: {user_message}\")\n",
        "\n",
        "    # Format as Phi-3 prompt\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are an expert Minecraft assistant with memory capabilities.<|end|>\n",
        "<|user|>\n",
        "{user_message}<|end|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Generate response\n",
        "        bot_response = generate_response(prompt, max_tokens=100)\n",
        "\n",
        "        # Clean up response\n",
        "        if \"<|end|>\" in bot_response:\n",
        "            bot_response = bot_response.split(\"<|end|>\")[0].strip()\n",
        "\n",
        "        # Remove any remaining special tokens\n",
        "        bot_response = bot_response.replace(\"<|assistant|>\", \"\").strip()\n",
        "\n",
        "        print(f\"Bot: {bot_response}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Clean up memory\n",
        "del model\n",
        "del base_model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting to GGUF"
      ],
      "metadata": {
        "id": "KI1aqq5R-BIu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZGHydM9ZWZV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import gc\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "ADAPTER_DRIVE = \"/content/drive/MyDrive/CraftAI/minecraft-phi3-bot/\"\n",
        "TEMP = \"/tmp\"\n",
        "ADAPTER_TEMP = f\"{TEMP}/adapters\"\n",
        "MERGED_TEMP = f\"{TEMP}/merged\"\n",
        "OUTPUT_GGUF = f\"{TEMP}/minecraft-bot-q4.gguf\"\n",
        "\n",
        "\n",
        "# Clean temp\n",
        "if os.path.exists(ADAPTER_TEMP):\n",
        "    shutil.rmtree(ADAPTER_TEMP)\n",
        "os.makedirs(ADAPTER_TEMP, exist_ok=True)\n",
        "\n",
        "required_files = [\n",
        "    \"adapter_config.json\",\n",
        "    \"adapter_model.safetensors\",\n",
        "    \"README.md\"\n",
        "]\n",
        "\n",
        "for filename in required_files:\n",
        "    src = os.path.join(ADAPTER_DRIVE, filename)\n",
        "    dst = os.path.join(ADAPTER_TEMP, filename)\n",
        "\n",
        "    if os.path.exists(src):\n",
        "        size_mb = os.path.getsize(src) / (1024*1024)\n",
        "        print(f\"   â€¢ {filename} ({size_mb:.1f}MB)\", end=\"\")\n",
        "        shutil.copy2(src, dst)\n",
        "    else:\n",
        "        print(f\"  {filename} (not found)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "BASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Base Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cpu\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# Adapters\n",
        "model = PeftModel.from_pretrained(model, ADAPTER_TEMP)\n",
        "\n",
        "# Merging weights\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "os.makedirs(MERGED_TEMP, exist_ok=True)\n",
        "model.save_pretrained(MERGED_TEMP, safe_serialization=True, max_shard_size=\"5GB\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.save_pretrained(MERGED_TEMP)\n",
        "\n",
        "# Free memory\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "aJ0lNv3w-Ry3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "!pip install -q huggingface_hub gguf\n",
        "\n",
        "LLAMA_TEMP = f\"{TEMP}/llama.cpp\"\n",
        "!git clone --quiet --depth 1 https://github.com/ggerganov/llama.cpp.git {LLAMA_TEMP}\n",
        "!pip install -q -r {LLAMA_TEMP}/requirements.txt\n",
        "\n",
        "# Convert to FP16\n",
        "fp16_temp = f\"{TEMP}/minecraft-fp16.gguf\"\n",
        "\n",
        "try:\n",
        "    !python {LLAMA_TEMP}/convert_hf_to_gguf.py {MERGED_TEMP} \\\n",
        "        --outfile {fp16_temp} \\\n",
        "        --outtype f16\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Conversion error: {e}\")\n",
        "    sys.exit(1)"
      ],
      "metadata": {
        "id": "6THnw5vD-la6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize to Q4_K_M\n",
        "%cd {LLAMA_TEMP}\n",
        "\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y build-essential cmake\n",
        "\n",
        "# Build llama.cpp\n",
        "!cmake -B build\n",
        "!cmake --build build --config Release -j $(nproc)\n",
        "\n",
        "QUANT_BIN = f\"{LLAMA_TEMP}/build/bin/llama-quantize\"\n",
        "\n",
        "!{QUANT_BIN} {fp16_temp} {OUTPUT_GGUF} Q4_K_M\n",
        "\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "vxF35shl_uou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydrive2\n",
        "\n",
        "from google.colab import files\n",
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file = drive.CreateFile({'title': 'minecraft-bot-q4.gguf'})\n",
        "file.SetContentFile(\"OUTPUT_GGUF\")\n",
        "file.Upload()\n",
        "\n",
        "print(\"Uploaded successfully!\")\n"
      ],
      "metadata": {
        "id": "_7JKPo935qRl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}